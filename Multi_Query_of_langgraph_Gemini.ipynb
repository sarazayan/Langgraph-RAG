{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "dIrqq7cdlea-",
        "outputId": "f47f235c-dc43-450b-e044-ff152b110ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dIrqq7cdlea-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain-google-genai"
      ],
      "metadata": {
        "id": "9wfUpKjhU1G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1caa474a-7814-4bf4-96d2-58df36c68d95"
      },
      "id": "9wfUpKjhU1G-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19969669-b47f-47f3-b6d4-f7b155434840",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19969669-b47f-47f3-b6d4-f7b155434840",
        "outputId": "9b2c09a0-8552-4015-a2ba-1332eb9fc20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m488.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUbmqaQJ2SDk",
        "outputId": "85f20c2e-18da-486f-cd5a-ac2ea592689a"
      },
      "id": "KUbmqaQJ2SDk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21",
      "metadata": {
        "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21"
      },
      "source": [
        "# Corrective RAG\n",
        "\n",
        "Self-reflection can enhance RAG, enabling correction of poor quality retrieval or generations.\n",
        "\n",
        "Several recent papers focus on this theme, but implementing the ideas can be tricky.\n",
        "\n",
        "Here we show how to implement self-reflective RAG using `Mistral` and `LangGraph`.\n",
        "\n",
        "We'll focus on ideas from one paper, `Corrective RAG (CRAG)` [here](https://arxiv.org/pdf/2401.15884.pdf).\n",
        "\n",
        "![Screenshot 2024-02-07 at 1.21.51 PM.png](attachment:a65940f9-5c51-4d7c-9ca1-ae576e4bb51a.png)\n",
        "\n",
        "## Setup\n",
        "\n",
        "### Using APIs\n",
        "\n",
        "* Set `MISTRAL_API_KEY` and set up Subscription to activate it.\n",
        "* Set `TAVILY_API_KEY` to enable web search [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "### Using CoLab\n",
        "\n",
        "* [Here](https://colab.research.google.com/drive/1U5OcwWjoXZSud30q4XOk1UlIJNjaD3kX?usp=sharing) is a link to a CoLab for this notebook.\n",
        "\n",
        "### Running Locally\n",
        "\n",
        "#### Embeddings\n",
        "\n",
        "There are several options for local embeddings.\n",
        "\n",
        "(1) You can use `GPT4AllEmbeddings()` from Nomic.\n",
        "\n",
        "(2) You can also use Nomic's recently released [v1](https://blog.nomic.ai/posts/nomic-embed-text-v1) and [v1.5](https://blog.nomic.ai/posts/nomic-embed-matryoshka) embeddings.\n",
        "\n",
        "For these, simply:\n",
        "\n",
        "Clone [`llama.cpp`](https://github.com/ggerganov/llama.cpp):\n",
        "\n",
        "```\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "```\n",
        "\n",
        "Download GGUF weights for Nomic's embedding model(s), allowing them to be run locally:\n",
        "\n",
        "* https://huggingface.co/nomic-ai/nomic-embed-text-v1-GGUF\n",
        "* https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF\n",
        "\n",
        "Add to `llama.cpp/model` directory.\n",
        "\n",
        "Build llama.cpp:\n",
        "```\n",
        "cd llama.cpp\n",
        "make\n",
        "```\n",
        "\n",
        "### LLM\n",
        "\n",
        "(1) Download [Ollama app](https://ollama.ai/).\n",
        "\n",
        "(2) Download a `Mistral` model from various Mistral versions [here](https://ollama.ai/library/mistral) and Mixtral versions [here](https://ollama.ai/library/mixtral) available.\n",
        "```\n",
        "ollama pull mistral:instruct\n",
        "```\n",
        "\n",
        "### Tracing\n",
        "\n",
        "* Optionally, use [LangSmith](https://docs.smith.langchain.com/) for tracing (shown at bottom) by setting:\n",
        "\n",
        "```\n",
        "export LANGCHAIN_TRACING_V2=true\n",
        "export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "export LANGCHAIN_API_KEY=<your-api-key>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmdKQ1Wita0D",
        "outputId": "014ed96c-10d5-4afe-81a7-6777b4a3ae7f"
      },
      "id": "VmdKQ1Wita0D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt4all in /usr/local/lib/python3.10/dist-packages (2.2.1.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR-9cQ9_hiHA",
        "outputId": "b9a38f24-ce0d-45b4-c1f4-305dc9a8468e"
      },
      "id": "fR-9cQ9_hiHA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this cell and paste the API key in the prompt\n",
        "import os\n",
        "import getpass\n",
        "os.environ['GOOGLE_API_KEY'] = getpass.getpass('Gemini API Key:')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvNDhTo2U_70",
        "outputId": "4705ca3e-f7e6-4aef-aed8-ddd3c7b6efa2"
      },
      "id": "uvNDhTo2U_70",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjKj7AuRwu96",
        "outputId": "5a0cf8d2-a2d7-4780-aa8e-d2a98c633a2a"
      },
      "id": "qjKj7AuRwu96",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `ollama run()'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama2:7b"
      ],
      "metadata": {
        "id": "ZgHo2Dl_Fqcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb41ce1-c15d-48b4-c057-fdd137e633db"
      },
      "id": "ZgHo2Dl_Fqcn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama2:7b\")\n",
        "llm.invoke(\"Write a proposal email for digital transformation.\")"
      ],
      "metadata": {
        "id": "Xa_fz8cZ_3Jk"
      },
      "id": "Xa_fz8cZ_3Jk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f644869-436e-4bf6-a267-b2465c7b5aef",
      "metadata": {
        "id": "9f644869-436e-4bf6-a267-b2465c7b5aef"
      },
      "outputs": [],
      "source": [
        "# Flags for running locally\n",
        "\n",
        "run_local = \"Yes\"\n",
        "# local_llm = \"mistral:instruct\"\n",
        "local_llm = \"llama2:7b\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCKZxZXyY34",
        "outputId": "b18a5b97-e475-4725-88c4-21857ee53244"
      },
      "id": "ZYCKZxZXyY34",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m204.8/290.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Llama2"
      ],
      "metadata": {
        "id": "UiBAYGbJH1aK"
      },
      "id": "UiBAYGbJH1aK"
    },
    {
      "cell_type": "markdown",
      "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0",
      "metadata": {
        "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "First, let's index a popular blog post on agents.\n",
        "\n",
        "We can use [Mistral embeddings](https://python.langchain.com/docs/integrations/text_embedding/mistralai).\n",
        "\n",
        "For local, we can use [GPT4All](https://python.langchain.com/docs/integrations/text_embedding/gpt4all), which is a CPU optimized SBERT model [here](https://docs.gpt4all.io/gpt4all_python_embedding.html).\n",
        "\n",
        "We'll use a local vectorstore, [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "jYluTPEfUxK-"
      },
      "id": "jYluTPEfUxK-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDT92IKAxoFX",
        "outputId": "1b48f7a6-2ca8-41c0-b85d-658739ff58ab"
      },
      "id": "SDT92IKAxoFX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "key = getpass('Enter the secret value: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LvZEljZSxbk",
        "outputId": "998a8781-5580-4dea-8135-cbf66c405aa1"
      },
      "id": "5LvZEljZSxbk",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "# Load the configuration file\n",
        "with open(\"/content/config.yaml\") as f:\n",
        "    config = yaml.safe_load(f)"
      ],
      "metadata": {
        "id": "GK1z06lG0gF5"
      },
      "id": "GK1z06lG0gF5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.embeddings import LlamaCppEmbeddings\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.vectorstores import Chroma\n",
        "import json\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import re\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "\n",
        "# Load\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader=PyPDFDirectoryLoader(\"/content/\")\n",
        "docs=loader.load()\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7, top_p=0.85)\n",
        "\n",
        "\n",
        "#tokenSplit\n",
        "token_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=100\n",
        ")\n",
        "all_splits = token_text_splitter.split_documents(docs)\n",
        "\n",
        "#charactersplitter\n",
        "\n",
        "\n",
        "embedding = GooglePalmEmbeddings(google_api_key=key)\n",
        "#embedding = GPT4AllEmbeddings()\n",
        "    # Nomic v1 or v1.5\n",
        "    # embd_model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/nomic-embd/nomic-embed-text-v1.Q4_K_S.gguf\"\n",
        "    # embedding = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
        "\n",
        "\n",
        "# Index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embedding,\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "#bm25_retriever = BM25Retriever.from_documents(docs)\n",
        "#bm25_retriever.k = 2\n",
        "\n",
        "emb_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# initialize the ensemble retriever\n",
        "#ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, emb_retriever], weights=[0.5, 0.5])\n",
        "#Initialize the multi_query_retriver\n",
        "multi_query = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
        "#unique_docs = multi_query.get_relevant_documents(query=question)"
      ],
      "metadata": {
        "id": "b6TbXVk5lRvl"
      },
      "id": "b6TbXVk5lRvl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xmqQGGqnSxEc"
      },
      "id": "xmqQGGqnSxEc"
    },
    {
      "cell_type": "markdown",
      "id": "fe7fd10a-f64a-48de-a116-6d5890def1af",
      "metadata": {
        "id": "fe7fd10a-f64a-48de-a116-6d5890def1af"
      },
      "source": [
        "## Corrective RAG\n",
        "\n",
        "Let's implement self-reflective RAG with some ideas from the CRAG (Corrective RAG) [paper](https://arxiv.org/pdf/2401.15884.pdf):\n",
        "\n",
        "* Grade documents for relevance relative to the question.\n",
        "* If any are irrelevant, then we will supplement the context used for generation with web search.\n",
        "* For web search, we will re-phrase the question and use Tavily API.\n",
        "* We will then pass retrieved documents and web results to an LLM for final answer generation.\n",
        "\n",
        "Here is a schematic of our graph in more detail:\n",
        "\n",
        "![crag.png](attachment:a2fac558-b18e-4610-bfa7-0d40c92e0ede.png)\n",
        "\n",
        "We will implement this using [LangGraph](https://python.langchain.com/docs/langgraph):\n",
        "\n",
        "* See video [here](https://www.youtube.com/watch?ref=blog.langchain.dev&v=pbAd8O1Lvm4&feature=youtu.be)\n",
        "* See blog post [here](https://blog.langchain.dev/agentic-rag-with-langgraph/)\n",
        "\n",
        "---\n",
        "\n",
        "### State\n",
        "\n",
        "Every node in our graph will modify `state`, which is dict that contains values (`question`, `documents`, etc) relevant to RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e",
      "metadata": {
        "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Dict, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        keys: A dictionary where each key is a string.\n",
        "    \"\"\"\n",
        "\n",
        "    keys: Dict[str, any]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0081ff31-4a91-4dbc-9977-8bcdc7dd0aeb",
      "metadata": {
        "id": "0081ff31-4a91-4dbc-9977-8bcdc7dd0aeb"
      },
      "source": [
        "### Nodes and Edges\n",
        "\n",
        "Every node in the graph we laid out above is a function.\n",
        "\n",
        "Each node will modify the state in some way.\n",
        "\n",
        "Each edge will choose which node to call next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447d1333-082d-479a-a6fa-0ac0df78bb9d",
      "metadata": {
        "id": "447d1333-082d-479a-a6fa-0ac0df78bb9d"
      },
      "outputs": [],
      "source": [
        "### Nodes ###\n",
        "\n",
        "def token_splitter(state):\n",
        "    \"\"\"\n",
        "split loaded docs\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---TokenSplitter---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    local = state_dict[\"local\"]\n",
        "    token_text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=100)\n",
        "    all_splits = token_text_splitter.split_documents(docs)\n",
        "\n",
        "    return {\"keys\": {\"local\": local,\"vectorstore\":vectorstore ,\"question\": question}}\n",
        "\n",
        "def Character_splitter(state):\n",
        "    \"\"\"\n",
        "split loaded docs\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---CharacterSplitter---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    local = state_dict[\"local\"]\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=100)\n",
        "    all_splits = text_splitter.split_documents(docs)\n",
        "    return {\"keys\": {\"local\": local,\"vectorstore\":vectorstore ,\"question\": question}}\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    #vectorstore=state_dict[\"vectorstore\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    local = state_dict[\"local\"]\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "\n",
        "def ensemble_retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---Ensemble RETRIEVE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    vectorstore=state_dict[\"vectorstore\"]\n",
        "    local = state_dict[\"local\"]\n",
        "    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, emb_retriever], weights=[0.5, 0.5])\n",
        "    documents = ensemble_retriever.get_relevant_documents(question)\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "def multi_query_retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---Multi Query RETRIEVE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    #vectorstore=state_dict[\"vectorstore\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    multi_query = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
        "    documents = multi_query.get_relevant_documents(question)\n",
        "    #unique_docs = multi_query.get_relevant_documents(query=question)\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # Prompt\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\" please  answer the question asked by the user from the given {context}:\n",
        "        Question :{question} \"\"\",\n",
        "        input_variables=[\"context\",\"question\"],\n",
        "    )\n",
        "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7, top_p=0.85)\n",
        "\n",
        "\n",
        "\n",
        "    # Post-processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Run\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # Prompt\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\" You are an expert in answering questions related to Castle .\n",
        "        Your task is to answer employee queries on the provided Castle documentation.\n",
        "        Generate an answer for the given question based only on the provided documents {context} .\n",
        "        You should use enumerations in your answer for readability. put the resources at the end in a decent form .\n",
        "\n",
        "        If there is nothing in the context relevant to the question at hand, just say \"Sorry, \\\n",
        "        I could not find relevant information, please refer to a Castle expert\" Don't try to make up an answer.\n",
        "        if a user is using greeting words like\"hello , hi , howw are you , good morning\" , please answer back with Hi , How are you ?\n",
        "\n",
        "        Question: {question}\n",
        "        Helpful Answer:\n",
        "        \"\"\",\n",
        "        input_variables=[\"context\",\"question\"],\n",
        "    )\n",
        "    # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7, top_p=0.85)\n",
        "\n",
        "\n",
        "\n",
        "    # Post-processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Run\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "    }\n"
      ],
      "metadata": {
        "id": "KywdICSZEiwL"
      },
      "id": "KywdICSZEiwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6096626d-dfa5-48e0-8a24-3747b298bc67",
      "metadata": {
        "id": "6096626d-dfa5-48e0-8a24-3747b298bc67"
      },
      "source": [
        "## Build Graph\n",
        "\n",
        "This just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a63776c-f9cd-46ce-b8cf-95c066dc5b06",
      "metadata": {
        "id": "0a63776c-f9cd-46ce-b8cf-95c066dc5b06"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "multi_query_retrieve_enable = True\n",
        "\n",
        "if multi_query_retrieve_enable:\n",
        "\n",
        "  workflow.add_node(\"multi_query_retrieve\", multi_query_retrieve)\n",
        "  workflow.add_node(\"generate\", generate)\n",
        "\n",
        "  workflow.set_entry_point(\"multi_query_retrieve\")\n",
        "  workflow.add_edge(\"multi_query_retrieve\", \"generate\")\n",
        "\n",
        "  workflow.add_edge(\"generate\", END)\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "  app = workflow.compile()\n",
        "else:\n",
        "\n",
        "  # Define the nodes\n",
        "  workflow.add_node(\"token_splitter\", token_splitter)\n",
        "  workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "  workflow.add_node(\"generate\", generate)  # generatae\n",
        "\n",
        "  workflow.set_entry_point(\"token_splitter\")\n",
        "  workflow.add_edge(\"token_splitter\", \"retrieve\")\n",
        "  workflow.add_edge(\"retrieve\", \"generate\")\n",
        "  # workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "  # workflow.add_edge(\"web_search\", \"generate\")\n",
        "  workflow.add_edge(\"generate\", END)\n",
        "\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "'''"
      ],
      "metadata": {
        "id": "kpKvAc96lljT"
      },
      "id": "kpKvAc96lljT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if config[\"Indexing\"][\"SplitterType\"] == \"token\":"
      ],
      "metadata": {
        "id": "XzmJtgJl1SEh"
      },
      "id": "XzmJtgJl1SEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A1oXlOQM1nIo"
      },
      "id": "A1oXlOQM1nIo"
    },
    {
      "cell_type": "code",
      "source": [
        "''''\n",
        "import pprint\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "token_splitter_enable = False\n",
        "#character_splitter_enable= True\n",
        "\n",
        "if token_splitter_enable :\n",
        "  # Define the nodes\n",
        "  workflow.add_node(\"token_splitter\", token_splitter)\n",
        "  workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "  workflow.add_node(\"generate\", generate)  # generatae\n",
        "\n",
        "  workflow.set_entry_point(\"token_splitter\")\n",
        "  workflow.add_edge(\"token_splitter\", \"retrieve\")\n",
        "  workflow.add_edge(\"retrieve\", \"generate\")\n",
        "  # workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "  # workflow.add_edge(\"web_search\", \"generate\")\n",
        "  workflow.add_edge(\"generate\", END)\n",
        "# Compile\n",
        "  app = workflow.compile()\n",
        "\n",
        "\n",
        "elif config[\"Indexing\"][\"SplitterType\"] == \"characters\":\n",
        "\n",
        "  # Define the nodes\n",
        "  workflow.add_node(\"character_splitter\", Character_splitter)\n",
        "  workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "  workflow.add_node(\"generate\", generate)  # generatae\n",
        "\n",
        "  workflow.set_entry_point(\"character_splitter\")\n",
        "  workflow.add_edge(\"character_splitter\", \"retrieve\")\n",
        "  workflow.add_edge(\"retrieve\", \"generate\")\n",
        "  # workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "  # workflow.add_edge(\"web_search\", \"generate\")\n",
        "  workflow.add_edge(\"generate\", END)\n",
        "\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "'''"
      ],
      "metadata": {
        "id": "DNnV4znGvyvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "c98ecbc4-f47b-4526-9666-43067a6c9912"
      },
      "id": "DNnV4znGvyvg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\'\\nimport pprint\\n\\nfrom langgraph.graph import END, StateGraph\\n\\nworkflow = StateGraph(GraphState)\\ntoken_splitter_enable = False\\n#character_splitter_enable= True\\n\\nif token_splitter_enable :\\n  # Define the nodes\\n  workflow.add_node(\"token_splitter\", token_splitter)\\n  workflow.add_node(\"retrieve\", retrieve)  # retrieve\\n  workflow.add_node(\"generate\", generate)  # generatae\\n\\n  workflow.set_entry_point(\"token_splitter\")\\n  workflow.add_edge(\"token_splitter\", \"retrieve\")\\n  workflow.add_edge(\"retrieve\", \"generate\")\\n  # workflow.add_edge(\"transform_query\", \"web_search\")\\n  # workflow.add_edge(\"web_search\", \"generate\")\\n  workflow.add_edge(\"generate\", END)\\n# Compile\\n  app = workflow.compile()\\n\\n\\nelif config[\"Indexing\"][\"SplitterType\"] == \"characters\":\\n\\n  # Define the nodes\\n  workflow.add_node(\"character_splitter\", Character_splitter)\\n  workflow.add_node(\"retrieve\", retrieve)  # retrieve\\n  workflow.add_node(\"generate\", generate)  # generatae\\n\\n  workflow.set_entry_point(\"character_splitter\")\\n  workflow.add_edge(\"character_splitter\", \"retrieve\")\\n  workflow.add_edge(\"retrieve\", \"generate\")\\n  # workflow.add_edge(\"transform_query\", \"web_search\")\\n  # workflow.add_edge(\"web_search\", \"generate\")\\n  workflow.add_edge(\"generate\", END)\\n\\n\\n# Compile\\napp = workflow.compile()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "workflow = StateGraph(GraphState)\n",
        "#workflow.add_node(\"characters_splitter\", token_splitter)\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "# workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "# workflow.add_edge(\"web_search\", \"generate\")\"\"\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "dHefsP7PTSeR"
      },
      "id": "dHefsP7PTSeR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ac0a868a-f0f9-4aa9-b955-a78da2359af8",
      "metadata": {
        "id": "ac0a868a-f0f9-4aa9-b955-a78da2359af8"
      },
      "source": [
        "## Run\n",
        "\n",
        "`Mistral API -`\n",
        "\n",
        "Trace for below run: https://smith.langchain.com/public/0a5cbc97-a2f6-4697-856c-90a6302fd13e/r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab1d8df-a74e-4b48-a30b-e39bbfd5925a",
      "metadata": {
        "id": "3ab1d8df-a74e-4b48-a30b-e39bbfd5925a"
      },
      "outputs": [],
      "source": [
        "# # Run\n",
        "# inputs = {\n",
        "#     \"keys\": {\n",
        "#         \"question\": \"Explain how the different types of agent memory work?\",\n",
        "#         \"local\": run_local,\n",
        "#     }\n",
        "# }\n",
        "# for output in app.stream(inputs):\n",
        "#     for key, value in output.items():\n",
        "#         # Node\n",
        "#         pprint.pprint(f\"Node '{key}':\")\n",
        "#         # Optional: print full state at each node\n",
        "#         # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "#     pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# # Final generation\n",
        "# pprint.pprint(value[\"keys\"][\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#splittertrial"
      ],
      "metadata": {
        "id": "Jay7bA2FkMCG"
      },
      "id": "Jay7bA2FkMCG"
    },
    {
      "cell_type": "code",
      "source": [
        "if config[\"Indexing\"][\"SplitterType\"] == \"characters\":"
      ],
      "metadata": {
        "id": "JZ9l79aIkLZT"
      },
      "id": "JZ9l79aIkLZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"What are the steps of capturing a single video frame from a live ECU video in Castle 5 ?\",\n",
        "        \"local\": run_local,\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "ez6omlPhzhaI",
        "outputId": "664f2292-29cf-407a-ad48-875d4ab1cc05"
      },
      "id": "ez6omlPhzhaI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Multi Query RETRIEVE---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Node 'multi_query_retrieve':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "('1. Video frames are transmitted from the ECU / Camera on a pixel basis.\\n'\n",
            " '2. The ECU sends one (or half pixel) at a time synced with Pclk (or '\n",
            " 'Pclkx2).\\n'\n",
            " '3. Based on the color map scheme used, the ECU sends a whole pixel or half '\n",
            " 'pixel aligned with the Pclk or Pclkx2.\\n'\n",
            " '4. ECU serializer sends the video on a high speed GMSL (Gigabit Multimedia '\n",
            " 'Serial Link) line.\\n'\n",
            " '5. The GMSL line is a single serial line that has all video signals (Pclk, '\n",
            " 'Hsync, Vsync, DE, Vid_data_x8x16x24) serialized / encoded inside.\\n'\n",
            " '6. A video deserializer is the outer / first interface unit in the PIP that '\n",
            " 'decodes ECU video.\\n'\n",
            " \"7. The deserializer's main objective is to decode the serial data sent on \"\n",
            " 'the GMSL / FPD link and recover it back to the parallel form of data.\\n'\n",
            " '8. The deserializer provides one pixel at a time (or half pixel) synced with '\n",
            " 'the Pclk (or Pclkx2) ticks.\\n'\n",
            " '9. Video in core \"castle_v_vid_in_axi4s_v1_00_a\" accepts the sent pixels '\n",
            " 'from the deserializer and accumulates them to form a complete line.\\n'\n",
            " '10. The line is then pushed from the the video in core to VDMA over the AXI '\n",
            " 'Stream bus (one pixel at a time on AXIS Aclk = 100Mhz).\\n'\n",
            " '11. The VDMA is pre-configured in the FW initialization time with captured '\n",
            " 'video resolution (VDMA resolution config could be modified in runtime as '\n",
            " 'well).\\n'\n",
            " '12. VDMA sequentially accepts line after line from the Video in core.\\n'\n",
            " '13. VDMA is to form a complete frame (based on configured resolution) and '\n",
            " 'buffer it in SDram (in a one dimensional array) with the memory map '\n",
            " 'dedicated AXI4 bus interface.\\n'\n",
            " '14. Once a complete frame has been formed in the SDRam array, we can '\n",
            " 'retrieve it over Ethernet to PC.\\n'\n",
            " '15. This could be realized over a dedicated Netconn socket or over Make '\n",
            " 'Message PcCom.\\n'\n",
            " '16. Either way, the FPGA sends one line at a time (from the SDram array) to '\n",
            " 'a PC program (Java test case), and the program is supposed to aggregate '\n",
            " 'received lines together and store them in the windows File System.\\n'\n",
            " '\\n'\n",
            " 'Resources:\\n'\n",
            " '[Document '\n",
            " '1](/content/castle/1-building-a-video-frame-out-of-pixels-capture-feature-in-a-nutshell.pdf)\\n'\n",
            " '[Document 2](/content/castle/7-stream-capture-raw-videos.pdf)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \" ?\",\n",
        "        \"local\": run_local,\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ],
      "metadata": {
        "id": "FotPd3LuP7J3"
      },
      "id": "FotPd3LuP7J3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"hello ?\",\n",
        "        \"local\": run_local,\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "O5ui1rApQG6R",
        "outputId": "6c3893ca-74c8-49b6-bf4d-c138bc6dc681"
      },
      "id": "O5ui1rApQG6R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "'Hi, How are you?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0bDJoAk5JIG"
      },
      "id": "r0bDJoAk5JIG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}